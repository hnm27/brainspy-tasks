{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f503656",
   "metadata": {},
   "source": [
    "# Example: Advanced model customisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc96bbd3",
   "metadata": {},
   "source": [
    "This notebook covers an example on how to find functionality in hardware for a more complex task that requires more advanced usage of custom models. It will cover an example with the famous LeNet neural network architecture for the classical classification benchmark task of digit recognition with the Modified National Institute of Standards and Technology (MNIST) dataset.  \n",
    "\n",
    "\n",
    "**Prerequisites:**\n",
    "- You have a surrogate model from a DNPU model that is still connected to an NI device in the lab\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a752a873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"4.finding_functionality/cnn_main.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Image from https://www.codexa.net/cnn-mnist-keras-beginner/\n",
    "Image(url= \"4.finding_functionality/cnn_main.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4354a",
   "metadata": {},
   "source": [
    "## 1. Having a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b127914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unai/anaconda3/envs/notebooks/lib/python3.10/site-packages/tqdm-4.64.1-py3.10.egg/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Downloading dataset in ./tmp/data\n",
    "dataset_example = MNIST('./tmp/data', train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f19f5e",
   "metadata": {},
   "source": [
    "### 1.1 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2411c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./tmp/data\n",
       "    Split: Train"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8fe7500",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dataset_element = dataset_example[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57aa8402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(first_dataset_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf73a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28>, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_dataset_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a812bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_img = first_dataset_element[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83db534f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6880a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "first_img_np = np.array(first_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fed900e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_img_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "959eb6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_img_np.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58f99e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_img_np.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c267b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_img_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f5a7cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
       "        0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117647,\n",
       "        0.36862745, 0.60392157, 0.66666667, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235294, 0.6745098 ,\n",
       "        0.99215686, 0.94901961, 0.76470588, 0.25098039, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215686, 0.93333333, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.98431373, 0.36470588, 0.32156863,\n",
       "        0.32156863, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882353, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.77647059,\n",
       "        0.71372549, 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31372549, 0.61176471,\n",
       "        0.41960784, 0.99215686, 0.99215686, 0.80392157, 0.04313725,\n",
       "        0.        , 0.16862745, 0.60392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509804, 0.99215686, 0.74509804, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313725, 0.74509804, 0.99215686, 0.2745098 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.1372549 , 0.94509804, 0.88235294,\n",
       "        0.62745098, 0.42352941, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764706, 0.94117647,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
       "        0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.36470588, 0.98823529, 0.99215686, 0.73333333,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.97647059, 0.99215686, 0.97647059,\n",
       "        0.25098039, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980392, 0.71764706, 0.99215686, 0.99215686, 0.81176471,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.58039216, 0.89803922,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.71372549,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882353,\n",
       "        0.83529412, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.77647059, 0.31764706, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058824, 0.85882353, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.76470588, 0.31372549,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568627,\n",
       "        0.6745098 , 0.88627451, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156863, 0.04313725, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333333,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137255, 0.52941176,\n",
       "        0.51764706, 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_img_np/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e02b040",
   "metadata": {},
   "source": [
    "### 1.2 Image modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6380da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_values = []\n",
    "desired_label = 5\n",
    "for i in range(len(dataset_example)):\n",
    "    current = dataset_example[i]\n",
    "    if current[1] == desired_label:\n",
    "        labelled_values.append(np.array(current[0]))\n",
    "\n",
    "labelled_values = np.array(labelled_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "187c86d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5421, 28, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40b9d901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.imshow(labelled_values.mean(0),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54f4879",
   "metadata": {},
   "source": [
    "### 1.3 Labels and One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52aac755",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_label = first_dataset_element[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "170f8bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9948f292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_label_one_hot = np.zeros(10)\n",
    "first_label_one_hot[first_label] = 1\n",
    "first_label_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0314e66",
   "metadata": {},
   "source": [
    "Note that the model should also produce outputs that are a vector with the same dimension as the number of labels. In this case 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c05a5",
   "metadata": {},
   "source": [
    "## 2. Understanding loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895919fd",
   "metadata": {},
   "source": [
    "The cross entropy loss function can be decomposed into Softmax and Negative Likelihood. We can have a look at these concepts individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42d8b8",
   "metadata": {},
   "source": [
    "### 2.1 Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a1da1f",
   "metadata": {},
   "source": [
    "We want to represent the raw output of our model (logits) as a probability, for which it would need to satisfy the following conditions:\n",
    "- Each element in $\\hat{y_i}$ is in a range from 0 to 1. \n",
    "- All the elements of the output prediction vector $\\hat{y_i}$ add to 1.\n",
    "- It maintains this ratio when translating it into a probability. I.e. if a if one value in $\\hat{y_i}$ is greater, it should maintain its proportion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58255332",
   "metadata": {},
   "source": [
    "In principle, we could try to satisfy these conditions by dividing each of the elements in the output vector prediction $\\hat{y_i}$ by the sum of all the elements in the same vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6828fa4",
   "metadata": {},
   "source": [
    "$ f({y_i}) = \\frac{\\hat{y_{ij}}}{\\sum_{j=0}^{n} \\hat{y_{ij}}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ab8e161",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_i_hat = np.array([3,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff7d249c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.3333333333333333\n",
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "f = []\n",
    "for i in range(len(y_i_hat)):\n",
    "    f.append(y_i_hat[i] / y_i_hat.sum())\n",
    "    print(f[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cbde5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(f).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451ea3ca",
   "metadata": {},
   "source": [
    "However, if we have negative values, we could end with NaN values. In order to maintain numerical stability, we can turn all negative values into positive ones we use the function $e^x$. Making the softmax function formula be: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14be853",
   "metadata": {},
   "source": [
    "$  f({y_i}) = \\frac{e^{\\hat{y_{ij}}}}{\\sum_{j=0}^{n} e^{\\hat{y_{ij}}}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ffa26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6652409557748219\n",
      "0.24472847105479764\n",
      "0.09003057317038045\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = []\n",
    "for i in range(len(y_i_hat)):\n",
    "    f.append( np.exp(y_i_hat[i]) / np.exp(y_i_hat).sum())\n",
    "    print(f[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "608cb428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.array(f).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f79b43f",
   "metadata": {},
   "source": [
    "Where n is the number of classes in the dataset. $\\hat{y_i}$ is a prediction vector of the same size as the number of classes. The model should maintain the same vector lenghth as the target values. Remember that we have already seen an example of how target vectors look like for the number 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c71ccdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_label_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43d130",
   "metadata": {},
   "source": [
    "Finally, the division can be made more numerically stable (avoiding division by zero), when transformed into the log domain, where it would be a substraction. Therefore, \n",
    "it is not uncommon to use $log(f(y_i))$ instead. This other function is called log softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46217147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source from https://www.youtube.com/watch?v=RC_A9Tu99y4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf9b7a",
   "metadata": {},
   "source": [
    "### 2.2 Maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa493d",
   "metadata": {},
   "source": [
    "The goal of maximum likelihood is to find a distribution that maximises the likelihood of observing the outputs that are measured. Therefore, it assigns higher probabilities to models that are able to represent probabilities based on what was observed and picks the model that gives existing labels the highest probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee588067",
   "metadata": {},
   "source": [
    "To understand this better, let us assume that we have a simple perceptron  $\\hat{y}$ = $\\sigma$(W*x + b) that classifies four points (two blue and two red). Our particular prediction $\\hat{y}$ is equal to the probability of a particular point being labelled correctly $P$(blue) = $\\sigma$(W*x + b). Let us assume that you have two different models, giving you different probabilities for the same points. The first model is represented on the left of the image, and the second model is represented on the right of the image below. The images show the probability of being red in red, and the probability of being blue in blue. Note that the probability of being blue is one minus the probability of being red. Assuming that the points are independent events, then the probability for the whole arrangement is the product of the probabilities of each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ceb1d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"4.finding_functionality/likelihood.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Image and lesson from https://www.youtube.com/watch?v=6nUUeQ9AeUA\n",
    "Image(url= \"4.finding_functionality/likelihood.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc73e190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.779523573132869"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(np.log(0.6) + np.log(0.2) + np.log(0.1) + np.log(0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f49f1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1960046346767592"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(np.log(0.7) + np.log(0.9) + np.log(0.8) + np.log(0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f337f",
   "metadata": {},
   "source": [
    "### 2.3 Binary cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e66af9b",
   "metadata": {},
   "source": [
    "We want to maximise the likelihood of a model, but for this we need to multiply many points, which is inconvenient when we have huge datasets. In order to make products into sums, we use log functions. Probabilities are expressed in a range from 0 to 1, but on log scale these values are negative, so we take the negative log of the probabilities. Lower cross entropy is better than higher. The final formula for cross entropy is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55610e3e",
   "metadata": {},
   "source": [
    "$- \\sum_{i=0}^{m} y_i . ln(p_i) + (1 - y_i) . ln(1 - p_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1792cd2d",
   "metadata": {},
   "source": [
    "Where m is the number of elements in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751a179",
   "metadata": {},
   "source": [
    "### 2.4 Multi-class cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa2c2a2",
   "metadata": {},
   "source": [
    "In a very similar way, this concept can be extended to multi classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c7635b",
   "metadata": {},
   "source": [
    "$- \\sum_{j=0}^{n} \\sum_{i=0}^{m} y_{ij} . ln(p_{ij})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15728fcc",
   "metadata": {},
   "source": [
    "Where n is the number of classes in the dataset. $\\hat{y_i}$ is a prediction vector of the same size as the number of classes. The model should maintain the same vector lenghth as the target values. Remember that we have already seen an example of how target vectors look like for the number 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8cdfbfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_label_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb7eeba",
   "metadata": {},
   "source": [
    "## 3. Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10cd55",
   "metadata": {},
   "source": [
    "## 4. Preparing the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d096cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torch.jit.annotations import Optional\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 data_dir: str = \"./tmp/data\",\n",
    "                 batch_size: int = 32,\n",
    "                 num_workers=4,\n",
    "                 pin_memory=False,\n",
    "                 split=[55000, 5000]):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.split = split\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir,\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor())\n",
    "            self.mnist_train, self.mnist_val = random_split(\n",
    "                mnist_full, self.split)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir,\n",
    "                                    train=False,\n",
    "                                    transform=transforms.ToTensor())\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=self.pin_memory)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers,\n",
    "                          shuffle=False,\n",
    "                          pin_memory=self.pin_memory)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers,\n",
    "                          shuffle=False,\n",
    "                          pin_memory=self.pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "297b91fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmetrics.functional.classification.accuracy import accuracy\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "class TrainerMNIST(pl.LightningModule):\n",
    "    def __init__(self, model, batch_size, learning_rate=1e-3):\n",
    "        super(TrainerMNIST, self).__init__()\n",
    "        self.name = 'mnist_trainer'\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return opt\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def shared_step(self, batch):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = F.nll_loss(preds, y)\n",
    "        acc = accuracy(preds, y)\n",
    "        return loss, acc\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.shared_step(batch)\n",
    "        self.log('loss_epoch', {'train': loss}, on_step=False, on_epoch=True)\n",
    "        self.log('acc_epoch', {'train': acc}, on_step=False, on_epoch=True)\n",
    "        self.log('train_loss', loss, prog_bar=False)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self.shared_step(batch)\n",
    "        self.log('loss_epoch', {'val': loss}, on_step=False, on_epoch=True)\n",
    "        self.log('acc_epoch', {'val': acc}, on_step=False, on_epoch=True)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, acc = self.shared_step(batch)\n",
    "        self.log('loss_epoch', {'test': loss}, on_step=False, on_epoch=True)\n",
    "        self.log('acc_epoch', {'test': acc}, on_step=False, on_epoch=True)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    # Allow brains-py to do constrained optimisation\n",
    "    def training_step_end(self, outputs):\n",
    "        if 'constraint_weights' in dir(self.model):\n",
    "            self.model.constraint_weights()\n",
    "        return outputs\n",
    "    \n",
    "    # Make pretty the progress bar\n",
    "    def get_progress_bar_dict(self):\n",
    "        # don't show the version number\n",
    "        items = super().get_progress_bar_dict()\n",
    "        items.pop(\"v_num\", None)\n",
    "        return items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fba408a",
   "metadata": {},
   "source": [
    "## 5. Creating a vanilla pytorch model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f69e63",
   "metadata": {},
   "source": [
    "A custom model can be created in pytorch by implementing a child class of nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9be29de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"4.finding_functionality/lenet3.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "Image(url= \"4.finding_functionality/lenet3.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1944661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FlattenView(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenView, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #return x.view(x.shape[0], -1)\n",
    "        return x.flatten(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "328cce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    \n",
    "    # Constructor: Initialisation stage\n",
    "    def __init__(self, mode='nn'):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.name = 'LeNet'\n",
    "\n",
    "        # First convolution layer\n",
    "        #                     (in_channels, out_channels, kernel_size)\n",
    "        self.conv1 = nn.Conv2d(1          ,6            , 3)\n",
    "        \n",
    "        \n",
    "        # Second convolution layer\n",
    "        #                     (in_channels, out_channels, kernel_size)\n",
    "        self.conv2 = nn.Conv2d(6          ,16           , 3)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Sequential(FlattenView(), \n",
    "                                nn.Linear(400, 120),\n",
    "                                nn.ReLU(), \n",
    "                                nn.Linear(120, 84), \n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(84, 10))\n",
    "        # Activation function\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x # Logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db71f353",
   "metadata": {},
   "source": [
    "## 6. Training the vanilla model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a44c5",
   "metadata": {},
   "source": [
    "### Hyperparemeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0568b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2efd6c4",
   "metadata": {},
   "source": [
    "### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c05e5061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2508835240\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "seed = 2508835240\n",
    "seed = seed_everything(seed, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c25fff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNISTDataModule(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5eb797d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_acc', filename='sample-mnist-{val_acc:.3f}', mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "458591be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afad657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper = TrainerMNIST(model, batch_size=batch_size, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6290a488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unai/anaconda3/envs/notebooks/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/unai/anaconda3/envs/notebooks/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1764: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=0,max_epochs=5,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f0c70fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e291fc03a94b52ef\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e291fc03a94b52ef\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9bfe858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /home/unai/Documents/3-Programming/bspy/tasks/notebooks/lightning_logs\n",
      "\n",
      "  | Name  | Type  | Params\n",
      "--------------------------------\n",
      "0 | model | LeNet | 60.1 K\n",
      "--------------------------------\n",
      "60.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.1 K    Total params\n",
      "0.240     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  92%|| 860/939 [00:09<00:00, 89.68it/s, loss=0.152, v_num=0, train_acc=0.958]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                  | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                     | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  92%|| 861/939 [00:09<00:00, 88.66it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  92%|| 862/939 [00:09<00:00, 88.66it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  92%|| 863/939 [00:09<00:00, 88.68it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  92%|| 864/939 [00:09<00:00, 88.71it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  92%|| 865/939 [00:09<00:00, 88.73it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  92%|| 866/939 [00:09<00:00, 88.76it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  92%|| 867/939 [00:09<00:00, 88.80it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  92%|| 868/939 [00:09<00:00, 88.84it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  93%|| 869/939 [00:09<00:00, 88.84it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  93%|| 870/939 [00:09<00:00, 88.88it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  93%|| 871/939 [00:09<00:00, 88.87it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  93%|| 872/939 [00:09<00:00, 88.90it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  93%|| 873/939 [00:09<00:00, 88.93it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  93%|| 874/939 [00:09<00:00, 88.94it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  93%|| 875/939 [00:09<00:00, 88.97it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  93%|| 876/939 [00:09<00:00, 88.99it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  93%|| 877/939 [00:09<00:00, 89.02it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  94%|| 878/939 [00:09<00:00, 89.05it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  94%|| 879/939 [00:09<00:00, 89.08it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  94%|| 880/939 [00:09<00:00, 89.09it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  94%|| 881/939 [00:09<00:00, 89.13it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  94%|| 882/939 [00:09<00:00, 89.15it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  94%|| 883/939 [00:09<00:00, 89.18it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  94%|| 884/939 [00:09<00:00, 89.19it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  94%|| 885/939 [00:09<00:00, 89.23it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  94%|| 886/939 [00:09<00:00, 89.23it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  94%|| 887/939 [00:09<00:00, 89.26it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  95%|| 888/939 [00:09<00:00, 89.27it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  95%|| 889/939 [00:09<00:00, 89.28it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  95%|| 890/939 [00:09<00:00, 89.30it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  95%|| 891/939 [00:09<00:00, 89.33it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  95%|| 892/939 [00:09<00:00, 89.37it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  95%|| 893/939 [00:09<00:00, 89.38it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  95%|| 894/939 [00:09<00:00, 89.41it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  95%|| 895/939 [00:10<00:00, 89.44it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  95%|| 896/939 [00:10<00:00, 89.47it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  96%|| 897/939 [00:10<00:00, 89.50it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  96%|| 898/939 [00:10<00:00, 89.53it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  96%|| 899/939 [00:10<00:00, 89.56it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  96%|| 900/939 [00:10<00:00, 89.58it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  96%|| 901/939 [00:10<00:00, 89.61it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  96%|| 902/939 [00:10<00:00, 89.63it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  96%|| 903/939 [00:10<00:00, 89.66it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  96%|| 904/939 [00:10<00:00, 89.68it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  96%|| 905/939 [00:10<00:00, 89.71it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  96%|| 906/939 [00:10<00:00, 89.73it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  97%|| 907/939 [00:10<00:00, 89.76it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  97%|| 908/939 [00:10<00:00, 89.78it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  97%|| 909/939 [00:10<00:00, 89.81it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  97%|| 910/939 [00:10<00:00, 89.83it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  97%|| 911/939 [00:10<00:00, 89.87it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  97%|| 912/939 [00:10<00:00, 89.90it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  97%|| 913/939 [00:10<00:00, 89.92it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  97%|| 914/939 [00:10<00:00, 89.93it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  97%|| 915/939 [00:10<00:00, 89.95it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  98%|| 916/939 [00:10<00:00, 89.99it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  98%|| 917/939 [00:10<00:00, 90.01it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  98%|| 918/939 [00:10<00:00, 90.04it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  98%|| 919/939 [00:10<00:00, 90.07it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  98%|| 920/939 [00:10<00:00, 90.06it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  98%|| 921/939 [00:10<00:00, 90.08it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  98%|| 922/939 [00:10<00:00, 90.10it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  98%|| 923/939 [00:10<00:00, 90.12it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  98%|| 924/939 [00:10<00:00, 90.15it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  99%|| 925/939 [00:10<00:00, 90.17it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  99%|| 926/939 [00:10<00:00, 90.19it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  99%|| 927/939 [00:10<00:00, 90.22it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  99%|| 928/939 [00:10<00:00, 90.26it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  99%|| 929/939 [00:10<00:00, 90.26it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  99%|| 930/939 [00:10<00:00, 90.29it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  99%|| 931/939 [00:10<00:00, 90.32it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  99%|| 932/939 [00:10<00:00, 90.34it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  99%|| 933/939 [00:10<00:00, 90.36it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0:  99%|| 934/939 [00:10<00:00, 90.38it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0: 100%|| 935/939 [00:10<00:00, 90.41it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0: 100%|| 936/939 [00:10<00:00, 90.45it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0: 100%|| 937/939 [00:10<00:00, 90.49it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0: 100%|| 938/939 [00:10<00:00, 90.51it/s, loss=0.152, v_num=0, train_acc=0.958]\u001b[A\n",
      "Epoch 0: 100%|| 939/939 [00:10<00:00, 90.51it/s, loss=0.152, v_num=0, train_acc=0.958, va\u001b[A\n",
      "Epoch 1:  92%|| 860/939 [00:10<00:00, 83.76it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                  | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                     | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  92%|| 861/939 [00:10<00:00, 82.83it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  92%|| 862/939 [00:10<00:00, 82.84it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  92%|| 863/939 [00:10<00:00, 82.84it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  92%|| 864/939 [00:10<00:00, 82.88it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  92%|| 865/939 [00:10<00:00, 82.91it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  92%|| 866/939 [00:10<00:00, 82.95it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  92%|| 867/939 [00:10<00:00, 82.98it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  92%|| 868/939 [00:10<00:00, 83.01it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  93%|| 869/939 [00:10<00:00, 83.04it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  93%|| 870/939 [00:10<00:00, 83.07it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  93%|| 871/939 [00:10<00:00, 83.09it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  93%|| 872/939 [00:10<00:00, 83.12it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  93%|| 873/939 [00:10<00:00, 83.11it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  93%|| 874/939 [00:10<00:00, 83.14it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  93%|| 875/939 [00:10<00:00, 83.18it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  93%|| 876/939 [00:10<00:00, 83.21it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  93%|| 877/939 [00:10<00:00, 83.25it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  94%|| 878/939 [00:10<00:00, 83.28it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  94%|| 879/939 [00:10<00:00, 83.30it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  94%|| 880/939 [00:10<00:00, 83.33it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  94%|| 881/939 [00:10<00:00, 83.36it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  94%|| 882/939 [00:10<00:00, 83.38it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  94%|| 883/939 [00:10<00:00, 83.40it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  94%|| 884/939 [00:10<00:00, 83.43it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  94%|| 885/939 [00:10<00:00, 83.46it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  94%|| 886/939 [00:10<00:00, 83.49it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  94%|| 887/939 [00:10<00:00, 83.51it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  95%|| 888/939 [00:10<00:00, 83.54it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  95%|| 889/939 [00:10<00:00, 83.57it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  95%|| 890/939 [00:10<00:00, 83.58it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  95%|| 891/939 [00:10<00:00, 83.62it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  95%|| 892/939 [00:10<00:00, 83.64it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  95%|| 893/939 [00:10<00:00, 83.67it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  95%|| 894/939 [00:10<00:00, 83.70it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  95%|| 895/939 [00:10<00:00, 83.73it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  95%|| 896/939 [00:10<00:00, 83.76it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  96%|| 897/939 [00:10<00:00, 83.80it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  96%|| 898/939 [00:10<00:00, 83.83it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  96%|| 899/939 [00:10<00:00, 83.86it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  96%|| 900/939 [00:10<00:00, 83.89it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  96%|| 901/939 [00:10<00:00, 83.92it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  96%|| 902/939 [00:10<00:00, 83.94it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  96%|| 903/939 [00:10<00:00, 83.94it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  96%|| 904/939 [00:10<00:00, 83.96it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  96%|| 905/939 [00:10<00:00, 83.99it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  96%|| 906/939 [00:10<00:00, 84.01it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  97%|| 907/939 [00:10<00:00, 84.05it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  97%|| 908/939 [00:10<00:00, 84.06it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  97%|| 909/939 [00:10<00:00, 84.08it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  97%|| 910/939 [00:10<00:00, 84.11it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  97%|| 911/939 [00:10<00:00, 84.14it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  97%|| 912/939 [00:10<00:00, 84.16it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  97%|| 913/939 [00:10<00:00, 84.18it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  97%|| 914/939 [00:10<00:00, 84.21it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  97%|| 915/939 [00:10<00:00, 84.24it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  98%|| 916/939 [00:10<00:00, 84.26it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  98%|| 917/939 [00:10<00:00, 84.29it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  98%|| 918/939 [00:10<00:00, 84.32it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  98%|| 919/939 [00:10<00:00, 84.34it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  98%|| 920/939 [00:10<00:00, 84.37it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  98%|| 921/939 [00:10<00:00, 84.39it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  98%|| 922/939 [00:10<00:00, 84.43it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  98%|| 923/939 [00:10<00:00, 84.45it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  98%|| 924/939 [00:10<00:00, 84.49it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  99%|| 925/939 [00:10<00:00, 84.50it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  99%|| 926/939 [00:10<00:00, 84.53it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  99%|| 927/939 [00:10<00:00, 84.55it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  99%|| 928/939 [00:10<00:00, 84.57it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  99%|| 929/939 [00:10<00:00, 84.60it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  99%|| 930/939 [00:10<00:00, 84.63it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  99%|| 931/939 [00:10<00:00, 84.65it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  99%|| 932/939 [00:11<00:00, 84.68it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  99%|| 933/939 [00:11<00:00, 84.72it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1:  99%|| 934/939 [00:11<00:00, 84.75it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1: 100%|| 935/939 [00:11<00:00, 84.78it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1: 100%|| 936/939 [00:11<00:00, 84.82it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1: 100%|| 937/939 [00:11<00:00, 84.85it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1: 100%|| 938/939 [00:11<00:00, 84.88it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 1: 100%|| 939/939 [00:11<00:00, 84.90it/s, loss=0.0831, v_num=0, train_acc=0.958, v\u001b[A\n",
      "Epoch 2:  92%|| 860/939 [00:10<00:00, 80.00it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                  | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                     | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  92%|| 861/939 [00:10<00:00, 79.24it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  92%|| 862/939 [00:10<00:00, 79.27it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  92%|| 863/939 [00:10<00:00, 79.31it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  92%|| 864/939 [00:10<00:00, 79.33it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  92%|| 865/939 [00:10<00:00, 79.36it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  92%|| 866/939 [00:10<00:00, 79.39it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  92%|| 867/939 [00:10<00:00, 79.43it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  92%|| 868/939 [00:10<00:00, 79.46it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  93%|| 869/939 [00:10<00:00, 79.49it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  93%|| 870/939 [00:10<00:00, 79.51it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  93%|| 871/939 [00:10<00:00, 79.53it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  93%|| 872/939 [00:10<00:00, 79.57it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  93%|| 873/939 [00:10<00:00, 79.60it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  93%|| 874/939 [00:10<00:00, 79.61it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  93%|| 875/939 [00:10<00:00, 79.64it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  93%|| 876/939 [00:10<00:00, 79.67it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  93%|| 877/939 [00:11<00:00, 79.70it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  94%|| 878/939 [00:11<00:00, 79.74it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  94%|| 879/939 [00:11<00:00, 79.76it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  94%|| 880/939 [00:11<00:00, 79.79it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  94%|| 881/939 [00:11<00:00, 79.82it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  94%|| 882/939 [00:11<00:00, 79.84it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  94%|| 883/939 [00:11<00:00, 79.87it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  94%|| 884/939 [00:11<00:00, 79.90it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  94%|| 885/939 [00:11<00:00, 79.93it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  94%|| 886/939 [00:11<00:00, 79.95it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  94%|| 887/939 [00:11<00:00, 79.99it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  95%|| 888/939 [00:11<00:00, 80.02it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  95%|| 889/939 [00:11<00:00, 80.05it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  95%|| 890/939 [00:11<00:00, 80.07it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  95%|| 891/939 [00:11<00:00, 80.10it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  95%|| 892/939 [00:11<00:00, 80.12it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  95%|| 893/939 [00:11<00:00, 80.15it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  95%|| 894/939 [00:11<00:00, 80.17it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  95%|| 895/939 [00:11<00:00, 80.19it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  95%|| 896/939 [00:11<00:00, 80.20it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  96%|| 897/939 [00:11<00:00, 80.22it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  96%|| 898/939 [00:11<00:00, 80.25it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  96%|| 899/939 [00:11<00:00, 80.27it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  96%|| 900/939 [00:11<00:00, 80.29it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  96%|| 901/939 [00:11<00:00, 80.29it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  96%|| 902/939 [00:11<00:00, 80.32it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  96%|| 903/939 [00:11<00:00, 80.34it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  96%|| 904/939 [00:11<00:00, 80.37it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  96%|| 905/939 [00:11<00:00, 80.39it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  96%|| 906/939 [00:11<00:00, 80.41it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  97%|| 907/939 [00:11<00:00, 80.44it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  97%|| 908/939 [00:11<00:00, 80.46it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  97%|| 909/939 [00:11<00:00, 80.47it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  97%|| 910/939 [00:11<00:00, 80.50it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  97%|| 911/939 [00:11<00:00, 80.53it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  97%|| 912/939 [00:11<00:00, 80.55it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  97%|| 913/939 [00:11<00:00, 80.57it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  97%|| 914/939 [00:11<00:00, 80.60it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  97%|| 915/939 [00:11<00:00, 80.63it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  98%|| 916/939 [00:11<00:00, 80.65it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  98%|| 917/939 [00:11<00:00, 80.66it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  98%|| 918/939 [00:11<00:00, 80.69it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  98%|| 919/939 [00:11<00:00, 80.71it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  98%|| 920/939 [00:11<00:00, 80.74it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  98%|| 921/939 [00:11<00:00, 80.76it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  98%|| 922/939 [00:11<00:00, 80.76it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  98%|| 923/939 [00:11<00:00, 80.79it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  98%|| 924/939 [00:11<00:00, 80.82it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  99%|| 925/939 [00:11<00:00, 80.84it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  99%|| 926/939 [00:11<00:00, 80.86it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  99%|| 927/939 [00:11<00:00, 80.89it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  99%|| 928/939 [00:11<00:00, 80.91it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  99%|| 929/939 [00:11<00:00, 80.94it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  99%|| 930/939 [00:11<00:00, 80.96it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  99%|| 931/939 [00:11<00:00, 80.99it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  99%|| 932/939 [00:11<00:00, 81.02it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  99%|| 933/939 [00:11<00:00, 81.06it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2:  99%|| 934/939 [00:11<00:00, 81.09it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2: 100%|| 935/939 [00:11<00:00, 81.12it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2: 100%|| 936/939 [00:11<00:00, 81.14it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2: 100%|| 937/939 [00:11<00:00, 81.18it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2: 100%|| 938/939 [00:11<00:00, 81.20it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 2: 100%|| 939/939 [00:11<00:00, 81.21it/s, loss=0.0616, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  92%|| 860/939 [00:12<00:01, 69.66it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                  | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                     | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  92%|| 861/939 [00:12<00:01, 69.04it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  92%|| 862/939 [00:12<00:01, 69.05it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  92%|| 863/939 [00:12<00:01, 69.07it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  92%|| 864/939 [00:12<00:01, 69.09it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  92%|| 865/939 [00:12<00:01, 69.11it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  92%|| 866/939 [00:12<00:01, 69.13it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  92%|| 867/939 [00:12<00:01, 69.15it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  92%|| 868/939 [00:12<00:01, 69.17it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  93%|| 869/939 [00:12<00:01, 69.19it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  93%|| 870/939 [00:12<00:00, 69.20it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  93%|| 871/939 [00:12<00:00, 69.22it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  93%|| 872/939 [00:12<00:00, 69.24it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  93%|| 873/939 [00:12<00:00, 68.67it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  93%|| 874/939 [00:12<00:00, 68.71it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  93%|| 875/939 [00:12<00:00, 68.74it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  93%|| 876/939 [00:12<00:00, 68.77it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  93%|| 877/939 [00:12<00:00, 68.80it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  94%|| 878/939 [00:12<00:00, 68.83it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  94%|| 879/939 [00:12<00:00, 68.86it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  94%|| 880/939 [00:12<00:00, 68.89it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  94%|| 881/939 [00:12<00:00, 68.92it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  94%|| 882/939 [00:12<00:00, 68.95it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  94%|| 883/939 [00:12<00:00, 68.98it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  94%|| 884/939 [00:12<00:00, 69.00it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  94%|| 885/939 [00:12<00:00, 69.03it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  94%|| 886/939 [00:12<00:00, 69.06it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  94%|| 887/939 [00:12<00:00, 69.10it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  95%|| 888/939 [00:12<00:00, 69.12it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  95%|| 889/939 [00:12<00:00, 69.15it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  95%|| 890/939 [00:12<00:00, 69.17it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  95%|| 891/939 [00:12<00:00, 69.20it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  95%|| 892/939 [00:12<00:00, 69.21it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  95%|| 893/939 [00:12<00:00, 69.24it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  95%|| 894/939 [00:12<00:00, 69.26it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  95%|| 895/939 [00:12<00:00, 69.29it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  95%|| 896/939 [00:12<00:00, 69.32it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  96%|| 897/939 [00:12<00:00, 69.30it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  96%|| 898/939 [00:12<00:00, 69.32it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  96%|| 899/939 [00:12<00:00, 69.34it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  96%|| 900/939 [00:12<00:00, 69.36it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  96%|| 901/939 [00:12<00:00, 69.39it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  96%|| 902/939 [00:12<00:00, 69.41it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  96%|| 903/939 [00:13<00:00, 69.41it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  96%|| 904/939 [00:13<00:00, 69.40it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  96%|| 905/939 [00:13<00:00, 69.43it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  96%|| 906/939 [00:13<00:00, 69.46it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  97%|| 907/939 [00:13<00:00, 69.46it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  97%|| 908/939 [00:13<00:00, 69.48it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  97%|| 909/939 [00:13<00:00, 69.51it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  97%|| 910/939 [00:13<00:00, 69.52it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  97%|| 911/939 [00:13<00:00, 69.54it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  97%|| 912/939 [00:13<00:00, 69.57it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  97%|| 913/939 [00:13<00:00, 69.59it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  97%|| 914/939 [00:13<00:00, 69.62it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  97%|| 915/939 [00:13<00:00, 69.64it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  98%|| 916/939 [00:13<00:00, 69.66it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  98%|| 917/939 [00:13<00:00, 69.68it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  98%|| 918/939 [00:13<00:00, 69.70it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  98%|| 919/939 [00:13<00:00, 69.71it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  98%|| 920/939 [00:13<00:00, 69.73it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  98%|| 921/939 [00:13<00:00, 69.75it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  98%|| 922/939 [00:13<00:00, 69.77it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  98%|| 923/939 [00:13<00:00, 69.80it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  98%|| 924/939 [00:13<00:00, 69.82it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  99%|| 925/939 [00:13<00:00, 69.84it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  99%|| 926/939 [00:13<00:00, 69.86it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  99%|| 927/939 [00:13<00:00, 69.88it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  99%|| 928/939 [00:13<00:00, 69.91it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  99%|| 929/939 [00:13<00:00, 69.92it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  99%|| 930/939 [00:13<00:00, 69.94it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  99%|| 931/939 [00:13<00:00, 69.96it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  99%|| 932/939 [00:13<00:00, 69.99it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  99%|| 933/939 [00:13<00:00, 70.00it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  99%|| 934/939 [00:13<00:00, 70.03it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3: 100%|| 935/939 [00:13<00:00, 70.04it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3: 100%|| 936/939 [00:13<00:00, 70.07it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3: 100%|| 937/939 [00:13<00:00, 70.09it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3: 100%|| 938/939 [00:13<00:00, 70.10it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 3: 100%|| 939/939 [00:13<00:00, 70.11it/s, loss=0.04, v_num=0, train_acc=0.958, val\u001b[A\n",
      "Epoch 4:  92%|| 860/939 [00:12<00:01, 66.86it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                  | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                     | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  92%|| 861/939 [00:12<00:01, 66.27it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  92%|| 862/939 [00:13<00:01, 66.29it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  92%|| 863/939 [00:13<00:01, 66.31it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  92%|| 864/939 [00:13<00:01, 66.31it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  92%|| 865/939 [00:13<00:01, 66.32it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  92%|| 866/939 [00:13<00:01, 66.34it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  92%|| 867/939 [00:13<00:01, 66.35it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  92%|| 868/939 [00:13<00:01, 66.38it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  93%|| 869/939 [00:13<00:01, 66.39it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  93%|| 870/939 [00:13<00:01, 66.42it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  93%|| 871/939 [00:13<00:01, 66.43it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  93%|| 872/939 [00:13<00:01, 66.45it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  93%|| 873/939 [00:13<00:00, 66.47it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  93%|| 874/939 [00:13<00:00, 66.49it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  93%|| 875/939 [00:13<00:00, 66.48it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  93%|| 876/939 [00:13<00:00, 66.50it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  93%|| 877/939 [00:13<00:00, 66.52it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  94%|| 878/939 [00:13<00:00, 66.54it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  94%|| 879/939 [00:13<00:00, 66.56it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  94%|| 880/939 [00:13<00:00, 66.58it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  94%|| 881/939 [00:13<00:00, 66.60it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  94%|| 882/939 [00:13<00:00, 66.62it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  94%|| 883/939 [00:13<00:00, 66.64it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  94%|| 884/939 [00:13<00:00, 66.66it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  94%|| 885/939 [00:13<00:00, 66.67it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  94%|| 886/939 [00:13<00:00, 66.70it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  94%|| 887/939 [00:13<00:00, 66.72it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  95%|| 888/939 [00:13<00:00, 66.75it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  95%|| 889/939 [00:13<00:00, 66.77it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  95%|| 890/939 [00:13<00:00, 66.80it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  95%|| 891/939 [00:13<00:00, 66.80it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  95%|| 892/939 [00:13<00:00, 66.83it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  95%|| 893/939 [00:13<00:00, 66.84it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  95%|| 894/939 [00:13<00:00, 66.79it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  95%|| 895/939 [00:13<00:00, 66.80it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  95%|| 896/939 [00:13<00:00, 66.76it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  96%|| 897/939 [00:13<00:00, 66.78it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  96%|| 898/939 [00:13<00:00, 66.74it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  96%|| 899/939 [00:13<00:00, 66.76it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  96%|| 900/939 [00:13<00:00, 66.74it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  96%|| 901/939 [00:13<00:00, 66.77it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  96%|| 902/939 [00:13<00:00, 66.76it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  96%|| 903/939 [00:13<00:00, 66.79it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  96%|| 904/939 [00:13<00:00, 66.82it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  96%|| 905/939 [00:13<00:00, 66.84it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  96%|| 906/939 [00:13<00:00, 66.86it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  97%|| 907/939 [00:13<00:00, 66.89it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  97%|| 908/939 [00:13<00:00, 66.91it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  97%|| 909/939 [00:13<00:00, 66.93it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  97%|| 910/939 [00:13<00:00, 66.95it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  97%|| 911/939 [00:13<00:00, 66.97it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  97%|| 912/939 [00:13<00:00, 66.98it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  97%|| 913/939 [00:13<00:00, 67.00it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  97%|| 914/939 [00:13<00:00, 67.02it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  97%|| 915/939 [00:13<00:00, 67.04it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  98%|| 916/939 [00:13<00:00, 67.05it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  98%|| 917/939 [00:13<00:00, 67.07it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  98%|| 918/939 [00:13<00:00, 67.09it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  98%|| 919/939 [00:13<00:00, 67.11it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  98%|| 920/939 [00:13<00:00, 67.13it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  98%|| 921/939 [00:13<00:00, 67.16it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  98%|| 922/939 [00:13<00:00, 67.17it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  98%|| 923/939 [00:13<00:00, 67.19it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  98%|| 924/939 [00:13<00:00, 67.20it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  99%|| 925/939 [00:13<00:00, 67.22it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  99%|| 926/939 [00:13<00:00, 67.24it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  99%|| 927/939 [00:13<00:00, 67.26it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  99%|| 928/939 [00:13<00:00, 67.27it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  99%|| 929/939 [00:13<00:00, 67.29it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  99%|| 930/939 [00:13<00:00, 67.31it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  99%|| 931/939 [00:13<00:00, 67.33it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  99%|| 932/939 [00:13<00:00, 67.36it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  99%|| 933/939 [00:13<00:00, 67.38it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  99%|| 934/939 [00:13<00:00, 67.41it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4: 100%|| 935/939 [00:13<00:00, 67.44it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4: 100%|| 936/939 [00:13<00:00, 67.46it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4: 100%|| 937/939 [00:13<00:00, 67.47it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4: 100%|| 938/939 [00:13<00:00, 67.49it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4: 100%|| 939/939 [00:13<00:00, 67.49it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A\n",
      "Epoch 4: 100%|| 939/939 [00:13<00:00, 67.46it/s, loss=0.0542, v_num=0, train_acc=1.000, v\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 939/939 [00:13<00:00, 67.43it/s, loss=0.0542, v_num=0, train_acc=1.000, v\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model_wrapper, datamodule=mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b38d13d",
   "metadata": {},
   "source": [
    "## 6. Modifying LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "853d84ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"4.finding_functionality/lenetmodified.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "Image(url= \"4.finding_functionality/lenetmodified.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07ea51b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LeNetModified(nn.Module):\n",
    "    \n",
    "    # Constructor: Initialisation stage\n",
    "    def __init__(self, mode='nn'):\n",
    "        super(LeNetModified, self).__init__()\n",
    "        self.name = 'LeNetModified'\n",
    "\n",
    "        # First convolution layer\n",
    "        #                     (in_channels, out_channels, kernel_size)\n",
    "        self.conv1 = nn.Conv2d(1          ,6            , 3)\n",
    "        self.bn1 = nn.BatchNorm2d(6) # Same as out channels\n",
    "        \n",
    "        # Second convolution layer\n",
    "        #                     (in_channels, out_channels, kernel_size)\n",
    "        self.conv2 = nn.Conv2d(6          ,16           , 3)\n",
    "        self.bn2 = nn.BatchNorm2d(16) # Same as out channels\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Sequential(FlattenView(), \n",
    "                                nn.Linear(400, 120),\n",
    "                                nn.ReLU(), \n",
    "                                nn.Linear(120, 84), \n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(84, 10))\n",
    "        # Activation function\n",
    "        self.act = nn.Sigmoid() # Changed activation function\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x) # New line\n",
    "        x = self.act(x) \n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x) # New line\n",
    "        x = self.act(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x # Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "885e4640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unai/anaconda3/envs/notebooks/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/unai/anaconda3/envs/notebooks/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory /home/unai/Documents/3-Programming/bspy/tasks/notebooks/lightning_logs/version_0/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type          | Params\n",
      "----------------------------------------\n",
      "0 | model | LeNetModified | 60.1 K\n",
      "----------------------------------------\n",
      "60.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "60.1 K    Total params\n",
      "0.240     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  92%|| 860/939 [00:10<00:00, 85.98it/s, loss=0.185, v_num=1, train_acc=1.000]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                  | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                     | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  92%|| 861/939 [00:10<00:00, 84.50it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  92%|| 862/939 [00:10<00:00, 84.52it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  92%|| 863/939 [00:10<00:00, 84.55it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  92%|| 864/939 [00:10<00:00, 84.56it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  92%|| 865/939 [00:10<00:00, 84.59it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  92%|| 866/939 [00:10<00:00, 84.61it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  92%|| 867/939 [00:10<00:00, 84.63it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  92%|| 868/939 [00:10<00:00, 84.66it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  93%|| 869/939 [00:10<00:00, 84.69it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  93%|| 870/939 [00:10<00:00, 84.69it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  93%|| 871/939 [00:10<00:00, 84.73it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  93%|| 872/939 [00:10<00:00, 84.74it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  93%|| 873/939 [00:10<00:00, 84.76it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  93%|| 874/939 [00:10<00:00, 84.78it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  93%|| 875/939 [00:10<00:00, 84.81it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  93%|| 876/939 [00:10<00:00, 84.83it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  93%|| 877/939 [00:10<00:00, 84.84it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  94%|| 878/939 [00:10<00:00, 84.87it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  94%|| 879/939 [00:10<00:00, 84.86it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  94%|| 880/939 [00:10<00:00, 84.87it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  94%|| 881/939 [00:10<00:00, 84.90it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  94%|| 882/939 [00:10<00:00, 84.88it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  94%|| 883/939 [00:10<00:00, 84.92it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  94%|| 884/939 [00:10<00:00, 84.93it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  94%|| 885/939 [00:10<00:00, 84.96it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  94%|| 886/939 [00:10<00:00, 84.98it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  94%|| 887/939 [00:10<00:00, 85.00it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  95%|| 888/939 [00:10<00:00, 85.02it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  95%|| 889/939 [00:10<00:00, 85.01it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  95%|| 890/939 [00:10<00:00, 85.05it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  95%|| 891/939 [00:10<00:00, 85.06it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  95%|| 892/939 [00:10<00:00, 85.08it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  95%|| 893/939 [00:10<00:00, 85.11it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  95%|| 894/939 [00:10<00:00, 85.13it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  95%|| 895/939 [00:10<00:00, 85.15it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  95%|| 896/939 [00:10<00:00, 85.15it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  96%|| 897/939 [00:10<00:00, 85.18it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  96%|| 898/939 [00:10<00:00, 85.17it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  96%|| 899/939 [00:10<00:00, 85.19it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  96%|| 900/939 [00:10<00:00, 85.22it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  96%|| 901/939 [00:10<00:00, 85.24it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  96%|| 902/939 [00:10<00:00, 85.27it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  96%|| 903/939 [00:10<00:00, 85.26it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  96%|| 904/939 [00:10<00:00, 85.28it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  96%|| 905/939 [00:10<00:00, 85.30it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  96%|| 906/939 [00:10<00:00, 85.31it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  97%|| 907/939 [00:10<00:00, 85.33it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  97%|| 908/939 [00:10<00:00, 85.34it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  97%|| 909/939 [00:10<00:00, 85.37it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  97%|| 910/939 [00:10<00:00, 85.38it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  97%|| 911/939 [00:10<00:00, 85.38it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  97%|| 912/939 [00:10<00:00, 85.40it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  97%|| 913/939 [00:10<00:00, 85.40it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  97%|| 914/939 [00:10<00:00, 85.43it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  97%|| 915/939 [00:10<00:00, 85.43it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  98%|| 916/939 [00:10<00:00, 85.43it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  98%|| 917/939 [00:10<00:00, 85.46it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  98%|| 918/939 [00:10<00:00, 85.46it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  98%|| 919/939 [00:10<00:00, 85.48it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  98%|| 920/939 [00:10<00:00, 85.50it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  98%|| 921/939 [00:10<00:00, 85.52it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  98%|| 922/939 [00:10<00:00, 85.54it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  98%|| 923/939 [00:10<00:00, 85.53it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  98%|| 924/939 [00:10<00:00, 85.56it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  99%|| 925/939 [00:10<00:00, 85.57it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  99%|| 926/939 [00:10<00:00, 85.55it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  99%|| 927/939 [00:10<00:00, 85.59it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  99%|| 928/939 [00:10<00:00, 85.59it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  99%|| 929/939 [00:10<00:00, 85.62it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  99%|| 930/939 [00:10<00:00, 85.63it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  99%|| 931/939 [00:10<00:00, 85.65it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  99%|| 932/939 [00:10<00:00, 85.68it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  99%|| 933/939 [00:10<00:00, 85.70it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0:  99%|| 934/939 [00:10<00:00, 85.72it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0: 100%|| 935/939 [00:10<00:00, 85.71it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0: 100%|| 936/939 [00:10<00:00, 85.74it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0: 100%|| 937/939 [00:10<00:00, 85.76it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0: 100%|| 938/939 [00:10<00:00, 85.78it/s, loss=0.185, v_num=1, train_acc=1.000]\u001b[A\n",
      "Epoch 0: 100%|| 939/939 [00:10<00:00, 85.75it/s, loss=0.185, v_num=1, train_acc=1.000, va\u001b[A\n",
      "Epoch 1:  92%|| 860/939 [00:10<00:00, 84.91it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                  | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                     | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  92%|| 861/939 [00:10<00:00, 83.50it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  92%|| 862/939 [00:10<00:00, 83.52it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  92%|| 863/939 [00:10<00:00, 83.56it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  92%|| 864/939 [00:10<00:00, 83.56it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  92%|| 865/939 [00:10<00:00, 83.61it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  92%|| 866/939 [00:10<00:00, 83.63it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  92%|| 867/939 [00:10<00:00, 83.68it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  92%|| 868/939 [00:10<00:00, 83.68it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  93%|| 869/939 [00:10<00:00, 83.72it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  93%|| 870/939 [00:10<00:00, 83.74it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  93%|| 871/939 [00:10<00:00, 83.77it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  93%|| 872/939 [00:10<00:00, 83.80it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  93%|| 873/939 [00:10<00:00, 83.83it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  93%|| 874/939 [00:10<00:00, 83.85it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  93%|| 875/939 [00:10<00:00, 83.88it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  93%|| 876/939 [00:10<00:00, 83.91it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  93%|| 877/939 [00:10<00:00, 83.95it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  94%|| 878/939 [00:10<00:00, 83.96it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  94%|| 879/939 [00:10<00:00, 83.97it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  94%|| 880/939 [00:10<00:00, 84.00it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  94%|| 881/939 [00:10<00:00, 84.00it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  94%|| 882/939 [00:10<00:00, 84.03it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  94%|| 883/939 [00:10<00:00, 84.05it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  94%|| 884/939 [00:10<00:00, 84.07it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  94%|| 885/939 [00:10<00:00, 84.07it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  94%|| 886/939 [00:10<00:00, 84.08it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  94%|| 887/939 [00:10<00:00, 84.12it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  95%|| 888/939 [00:10<00:00, 84.12it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  95%|| 889/939 [00:10<00:00, 84.16it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  95%|| 890/939 [00:10<00:00, 84.18it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  95%|| 891/939 [00:10<00:00, 84.21it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  95%|| 892/939 [00:10<00:00, 84.24it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  95%|| 893/939 [00:10<00:00, 84.27it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  95%|| 894/939 [00:10<00:00, 84.30it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  95%|| 895/939 [00:10<00:00, 84.32it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  95%|| 896/939 [00:10<00:00, 84.35it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  96%|| 897/939 [00:10<00:00, 84.38it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  96%|| 898/939 [00:10<00:00, 84.40it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  96%|| 899/939 [00:10<00:00, 84.43it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  96%|| 900/939 [00:10<00:00, 84.44it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  96%|| 901/939 [00:10<00:00, 84.46it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  96%|| 902/939 [00:10<00:00, 84.49it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  96%|| 903/939 [00:10<00:00, 84.52it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  96%|| 904/939 [00:10<00:00, 84.55it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  96%|| 905/939 [00:10<00:00, 84.58it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  96%|| 906/939 [00:10<00:00, 84.60it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  97%|| 907/939 [00:10<00:00, 84.63it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  97%|| 908/939 [00:10<00:00, 84.66it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  97%|| 909/939 [00:10<00:00, 84.69it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  97%|| 910/939 [00:10<00:00, 84.71it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  97%|| 911/939 [00:10<00:00, 84.73it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  97%|| 912/939 [00:10<00:00, 84.76it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  97%|| 913/939 [00:10<00:00, 84.76it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  97%|| 914/939 [00:10<00:00, 84.79it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  97%|| 915/939 [00:10<00:00, 84.80it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  98%|| 916/939 [00:10<00:00, 84.82it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  98%|| 917/939 [00:10<00:00, 84.84it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  98%|| 918/939 [00:10<00:00, 84.86it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  98%|| 919/939 [00:10<00:00, 84.88it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  98%|| 920/939 [00:10<00:00, 84.89it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  98%|| 921/939 [00:10<00:00, 84.92it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  98%|| 922/939 [00:10<00:00, 84.92it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  98%|| 923/939 [00:10<00:00, 84.95it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  98%|| 924/939 [00:10<00:00, 84.97it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  99%|| 925/939 [00:10<00:00, 85.00it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  99%|| 926/939 [00:10<00:00, 85.01it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  99%|| 927/939 [00:10<00:00, 85.04it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  99%|| 928/939 [00:10<00:00, 85.07it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  99%|| 929/939 [00:10<00:00, 85.08it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  99%|| 930/939 [00:10<00:00, 85.09it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  99%|| 931/939 [00:10<00:00, 85.10it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  99%|| 932/939 [00:10<00:00, 85.13it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  99%|| 933/939 [00:10<00:00, 85.15it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1:  99%|| 934/939 [00:10<00:00, 85.19it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1: 100%|| 935/939 [00:10<00:00, 85.21it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1: 100%|| 936/939 [00:10<00:00, 85.24it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1: 100%|| 937/939 [00:10<00:00, 85.27it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1: 100%|| 938/939 [00:10<00:00, 85.30it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 1: 100%|| 939/939 [00:11<00:00, 85.28it/s, loss=0.15, v_num=1, train_acc=0.917, val\u001b[A\n",
      "Epoch 2:  92%|| 860/939 [00:09<00:00, 86.64it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                  | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                     | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  92%|| 861/939 [00:10<00:00, 85.16it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  92%|| 862/939 [00:10<00:00, 85.19it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  92%|| 863/939 [00:10<00:00, 85.20it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  92%|| 864/939 [00:10<00:00, 85.21it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  92%|| 865/939 [00:10<00:00, 85.25it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  92%|| 866/939 [00:10<00:00, 85.28it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  92%|| 867/939 [00:10<00:00, 85.30it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  92%|| 868/939 [00:10<00:00, 85.32it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  93%|| 869/939 [00:10<00:00, 85.36it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  93%|| 870/939 [00:10<00:00, 85.39it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  93%|| 871/939 [00:10<00:00, 85.42it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  93%|| 872/939 [00:10<00:00, 85.46it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  93%|| 873/939 [00:10<00:00, 85.49it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  93%|| 874/939 [00:10<00:00, 85.52it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  93%|| 875/939 [00:10<00:00, 85.53it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  93%|| 876/939 [00:10<00:00, 85.57it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  93%|| 877/939 [00:10<00:00, 85.60it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  94%|| 878/939 [00:10<00:00, 85.63it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  94%|| 879/939 [00:10<00:00, 85.65it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  94%|| 880/939 [00:10<00:00, 85.69it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  94%|| 881/939 [00:10<00:00, 85.70it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  94%|| 882/939 [00:10<00:00, 85.73it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  94%|| 883/939 [00:10<00:00, 85.76it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  94%|| 884/939 [00:10<00:00, 85.78it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  94%|| 885/939 [00:10<00:00, 85.80it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  94%|| 886/939 [00:10<00:00, 85.82it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  94%|| 887/939 [00:10<00:00, 85.84it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  95%|| 888/939 [00:10<00:00, 85.88it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  95%|| 889/939 [00:10<00:00, 85.89it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  95%|| 890/939 [00:10<00:00, 85.90it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  95%|| 891/939 [00:10<00:00, 85.93it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  95%|| 892/939 [00:10<00:00, 85.95it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  95%|| 893/939 [00:10<00:00, 85.97it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  95%|| 894/939 [00:10<00:00, 85.98it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  95%|| 895/939 [00:10<00:00, 86.00it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  95%|| 896/939 [00:10<00:00, 86.03it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  96%|| 897/939 [00:10<00:00, 86.04it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  96%|| 898/939 [00:10<00:00, 86.07it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  96%|| 899/939 [00:10<00:00, 86.10it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  96%|| 900/939 [00:10<00:00, 86.13it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  96%|| 901/939 [00:10<00:00, 86.12it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  96%|| 902/939 [00:10<00:00, 86.16it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  96%|| 903/939 [00:10<00:00, 86.16it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  96%|| 904/939 [00:10<00:00, 86.18it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  96%|| 905/939 [00:10<00:00, 86.20it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  96%|| 906/939 [00:10<00:00, 86.22it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  97%|| 907/939 [00:10<00:00, 86.25it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  97%|| 908/939 [00:10<00:00, 86.26it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  97%|| 909/939 [00:10<00:00, 86.29it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  97%|| 910/939 [00:10<00:00, 86.29it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  97%|| 911/939 [00:10<00:00, 86.31it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  97%|| 912/939 [00:10<00:00, 86.34it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  97%|| 913/939 [00:10<00:00, 86.35it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  97%|| 914/939 [00:10<00:00, 86.36it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  97%|| 915/939 [00:10<00:00, 86.36it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  98%|| 916/939 [00:10<00:00, 86.39it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  98%|| 917/939 [00:10<00:00, 86.41it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  98%|| 918/939 [00:10<00:00, 86.44it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  98%|| 919/939 [00:10<00:00, 86.46it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  98%|| 920/939 [00:10<00:00, 86.49it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  98%|| 921/939 [00:10<00:00, 86.51it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  98%|| 922/939 [00:10<00:00, 86.54it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  98%|| 923/939 [00:10<00:00, 86.56it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  98%|| 924/939 [00:10<00:00, 86.59it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  99%|| 925/939 [00:10<00:00, 86.61it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  99%|| 926/939 [00:10<00:00, 86.64it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  99%|| 927/939 [00:10<00:00, 86.66it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  99%|| 928/939 [00:10<00:00, 86.69it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  99%|| 929/939 [00:10<00:00, 86.70it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  99%|| 930/939 [00:10<00:00, 86.73it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  99%|| 931/939 [00:10<00:00, 86.75it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  99%|| 932/939 [00:10<00:00, 86.78it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  99%|| 933/939 [00:10<00:00, 86.80it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2:  99%|| 934/939 [00:10<00:00, 86.82it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2: 100%|| 935/939 [00:10<00:00, 86.84it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2: 100%|| 936/939 [00:10<00:00, 86.84it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2: 100%|| 937/939 [00:10<00:00, 86.88it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2: 100%|| 938/939 [00:10<00:00, 86.88it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 2: 100%|| 939/939 [00:10<00:00, 86.87it/s, loss=0.11, v_num=1, train_acc=0.958, val\u001b[A\n",
      "Epoch 3:  92%|| 860/939 [00:10<00:00, 85.49it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                  | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                     | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  92%|| 861/939 [00:10<00:00, 84.04it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  92%|| 862/939 [00:10<00:00, 84.00it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  92%|| 863/939 [00:10<00:00, 84.01it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  92%|| 864/939 [00:10<00:00, 84.02it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  92%|| 865/939 [00:10<00:00, 84.05it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  92%|| 866/939 [00:10<00:00, 84.08it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  92%|| 867/939 [00:10<00:00, 84.12it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  92%|| 868/939 [00:10<00:00, 84.14it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  93%|| 869/939 [00:10<00:00, 84.17it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  93%|| 870/939 [00:10<00:00, 84.20it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  93%|| 871/939 [00:10<00:00, 84.22it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  93%|| 872/939 [00:10<00:00, 84.24it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  93%|| 873/939 [00:10<00:00, 84.26it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  93%|| 874/939 [00:10<00:00, 84.27it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  93%|| 875/939 [00:10<00:00, 84.30it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  93%|| 876/939 [00:10<00:00, 84.33it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  93%|| 877/939 [00:10<00:00, 84.35it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  94%|| 878/939 [00:10<00:00, 84.37it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  94%|| 879/939 [00:10<00:00, 84.39it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  94%|| 880/939 [00:10<00:00, 84.42it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  94%|| 881/939 [00:10<00:00, 84.44it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  94%|| 882/939 [00:10<00:00, 84.47it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  94%|| 883/939 [00:10<00:00, 84.47it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  94%|| 884/939 [00:10<00:00, 84.50it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  94%|| 885/939 [00:10<00:00, 84.51it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  94%|| 886/939 [00:10<00:00, 84.53it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  94%|| 887/939 [00:10<00:00, 84.54it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  95%|| 888/939 [00:10<00:00, 84.54it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  95%|| 889/939 [00:10<00:00, 84.56it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  95%|| 890/939 [00:10<00:00, 84.57it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  95%|| 891/939 [00:10<00:00, 84.58it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  95%|| 892/939 [00:10<00:00, 84.61it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  95%|| 893/939 [00:10<00:00, 84.60it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  95%|| 894/939 [00:10<00:00, 84.61it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  95%|| 895/939 [00:10<00:00, 84.64it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  95%|| 896/939 [00:10<00:00, 84.66it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  96%|| 897/939 [00:10<00:00, 84.68it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  96%|| 898/939 [00:10<00:00, 84.68it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  96%|| 899/939 [00:10<00:00, 84.71it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  96%|| 900/939 [00:10<00:00, 84.73it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  96%|| 901/939 [00:10<00:00, 84.74it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  96%|| 902/939 [00:10<00:00, 84.76it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  96%|| 903/939 [00:10<00:00, 84.76it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  96%|| 904/939 [00:10<00:00, 84.77it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  96%|| 905/939 [00:10<00:00, 84.80it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  96%|| 906/939 [00:10<00:00, 84.82it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  97%|| 907/939 [00:10<00:00, 84.84it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  97%|| 908/939 [00:10<00:00, 84.83it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  97%|| 909/939 [00:10<00:00, 84.85it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  97%|| 910/939 [00:10<00:00, 84.87it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  97%|| 911/939 [00:10<00:00, 84.88it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  97%|| 912/939 [00:10<00:00, 84.90it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  97%|| 913/939 [00:10<00:00, 84.92it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  97%|| 914/939 [00:10<00:00, 84.93it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  97%|| 915/939 [00:10<00:00, 84.94it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  98%|| 916/939 [00:10<00:00, 84.96it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  98%|| 917/939 [00:10<00:00, 84.97it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  98%|| 918/939 [00:10<00:00, 84.97it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  98%|| 919/939 [00:10<00:00, 84.99it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  98%|| 920/939 [00:10<00:00, 85.00it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  98%|| 921/939 [00:10<00:00, 85.03it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  98%|| 922/939 [00:10<00:00, 85.05it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  98%|| 923/939 [00:10<00:00, 85.07it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  98%|| 924/939 [00:10<00:00, 85.10it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  99%|| 925/939 [00:10<00:00, 85.11it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  99%|| 926/939 [00:10<00:00, 85.13it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  99%|| 927/939 [00:10<00:00, 85.13it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  99%|| 928/939 [00:10<00:00, 85.16it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  99%|| 929/939 [00:10<00:00, 85.17it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  99%|| 930/939 [00:10<00:00, 85.19it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  99%|| 931/939 [00:10<00:00, 85.22it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  99%|| 932/939 [00:10<00:00, 85.25it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  99%|| 933/939 [00:10<00:00, 85.27it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3:  99%|| 934/939 [00:10<00:00, 85.29it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3: 100%|| 935/939 [00:10<00:00, 85.32it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3: 100%|| 936/939 [00:10<00:00, 85.31it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3: 100%|| 937/939 [00:10<00:00, 85.34it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3: 100%|| 938/939 [00:10<00:00, 85.33it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 3: 100%|| 939/939 [00:11<00:00, 85.31it/s, loss=0.0594, v_num=1, train_acc=1.000, v\u001b[A\n",
      "Epoch 4:  92%|| 860/939 [00:10<00:00, 79.46it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                  | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                     | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  92%|| 861/939 [00:11<00:01, 77.96it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  92%|| 862/939 [00:11<00:00, 77.98it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  92%|| 863/939 [00:11<00:00, 78.01it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  92%|| 864/939 [00:11<00:00, 78.01it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  92%|| 865/939 [00:11<00:00, 78.03it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  92%|| 866/939 [00:11<00:00, 78.06it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  92%|| 867/939 [00:11<00:00, 78.09it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  92%|| 868/939 [00:11<00:00, 78.12it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  93%|| 869/939 [00:11<00:00, 78.16it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  93%|| 870/939 [00:11<00:00, 78.20it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  93%|| 871/939 [00:11<00:00, 78.22it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  93%|| 872/939 [00:11<00:00, 78.25it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  93%|| 873/939 [00:11<00:00, 78.27it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  93%|| 874/939 [00:11<00:00, 78.29it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  93%|| 875/939 [00:11<00:00, 78.30it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  93%|| 876/939 [00:11<00:00, 78.31it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  93%|| 877/939 [00:11<00:00, 78.35it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  94%|| 878/939 [00:11<00:00, 78.38it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  94%|| 879/939 [00:11<00:00, 78.41it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  94%|| 880/939 [00:11<00:00, 78.43it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  94%|| 881/939 [00:11<00:00, 78.45it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  94%|| 882/939 [00:11<00:00, 78.48it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  94%|| 883/939 [00:11<00:00, 78.48it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  94%|| 884/939 [00:11<00:00, 78.51it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  94%|| 885/939 [00:11<00:00, 78.53it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  94%|| 886/939 [00:11<00:00, 78.55it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  94%|| 887/939 [00:11<00:00, 78.58it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  95%|| 888/939 [00:11<00:00, 78.61it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  95%|| 889/939 [00:11<00:00, 78.63it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  95%|| 890/939 [00:11<00:00, 78.64it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  95%|| 891/939 [00:11<00:00, 78.67it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  95%|| 892/939 [00:11<00:00, 78.68it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  95%|| 893/939 [00:11<00:00, 78.71it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  95%|| 894/939 [00:11<00:00, 78.73it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  95%|| 895/939 [00:11<00:00, 78.72it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  95%|| 896/939 [00:11<00:00, 78.75it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  96%|| 897/939 [00:11<00:00, 78.78it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  96%|| 898/939 [00:11<00:00, 78.81it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  96%|| 899/939 [00:11<00:00, 78.83it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  96%|| 900/939 [00:11<00:00, 78.85it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  96%|| 901/939 [00:11<00:00, 78.89it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  96%|| 902/939 [00:11<00:00, 78.88it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  96%|| 903/939 [00:11<00:00, 78.91it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  96%|| 904/939 [00:11<00:00, 78.93it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  96%|| 905/939 [00:11<00:00, 78.95it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  96%|| 906/939 [00:11<00:00, 78.98it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  97%|| 907/939 [00:11<00:00, 79.00it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  97%|| 908/939 [00:11<00:00, 79.03it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  97%|| 909/939 [00:11<00:00, 79.05it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  97%|| 910/939 [00:11<00:00, 79.08it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  97%|| 911/939 [00:11<00:00, 79.08it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  97%|| 912/939 [00:11<00:00, 79.11it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  97%|| 913/939 [00:11<00:00, 79.14it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  97%|| 914/939 [00:11<00:00, 79.17it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  97%|| 915/939 [00:11<00:00, 79.20it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  98%|| 916/939 [00:11<00:00, 79.22it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  98%|| 917/939 [00:11<00:00, 79.25it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  98%|| 918/939 [00:11<00:00, 79.28it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  98%|| 919/939 [00:11<00:00, 79.30it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  98%|| 920/939 [00:11<00:00, 79.30it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  98%|| 921/939 [00:11<00:00, 79.33it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  98%|| 922/939 [00:11<00:00, 79.35it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  98%|| 923/939 [00:11<00:00, 79.39it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  98%|| 924/939 [00:11<00:00, 79.41it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  99%|| 925/939 [00:11<00:00, 79.44it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  99%|| 926/939 [00:11<00:00, 79.45it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  99%|| 927/939 [00:11<00:00, 79.48it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  99%|| 928/939 [00:11<00:00, 79.51it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  99%|| 929/939 [00:11<00:00, 79.55it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  99%|| 930/939 [00:11<00:00, 79.57it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  99%|| 931/939 [00:11<00:00, 79.60it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  99%|| 932/939 [00:11<00:00, 79.64it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  99%|| 933/939 [00:11<00:00, 79.67it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4:  99%|| 934/939 [00:11<00:00, 79.70it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4: 100%|| 935/939 [00:11<00:00, 79.74it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4: 100%|| 936/939 [00:11<00:00, 79.76it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4: 100%|| 937/939 [00:11<00:00, 79.80it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4: 100%|| 938/939 [00:11<00:00, 79.81it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4: 100%|| 939/939 [00:11<00:00, 79.81it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A\n",
      "Epoch 4: 100%|| 939/939 [00:11<00:00, 79.78it/s, loss=0.0869, v_num=1, train_acc=0.958, v\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 939/939 [00:11<00:00, 79.75it/s, loss=0.0869, v_num=1, train_acc=0.958, v\n"
     ]
    }
   ],
   "source": [
    "model_modified = LeNetModified()\n",
    "model_modified_wrapper = TrainerMNIST(model_modified, batch_size=batch_size, learning_rate=lr)\n",
    "trainer = pl.Trainer(gpus=1,max_epochs=5,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    deterministic=True)\n",
    "trainer.fit(model_modified_wrapper, datamodule=mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e0de4",
   "metadata": {},
   "source": [
    "For this exercise, the Genetic Algorithm will be used, as this is a fast way to explore functionality on devices directly on-chip. The particularities of each config is explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5772b3c0",
   "metadata": {},
   "source": [
    "## 7. Using convolutions with DNPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f112a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_configs():\n",
    "    configs = {}\n",
    "    configs[\"model_data_path\"] = \"./training_data.pt\"  # \"./tmp/models/training_data_new_without_clipping.pt\"  #\"./tmp/models/training_data.pt\"\n",
    "    configs[\"processor_type\"] = \"simulation\"\n",
    "    configs[\"waveform\"] = {}\n",
    "    configs[\"waveform\"][\"plateau_length\"] = 1\n",
    "    configs[\"waveform\"][\"slope_length\"] = 0\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24c4c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from brainspy.processors.processor import Processor\n",
    "from brainspy.processors.modules.conv import DNPUConv2d\n",
    "\n",
    "class LeNetDNPU(nn.Module):\n",
    "    \n",
    "    # Constructor: Initialisation stage\n",
    "    def __init__(self, mode='nn'):\n",
    "        super(LeNetDNPU, self).__init__()\n",
    "        self.name = 'LeNetDNPU'\n",
    "        \n",
    "        ###############################################################\n",
    "        # Declare a processor\n",
    "        configs = get_node_configs()\n",
    "        model_data = torch.load(configs['model_data_path'])\n",
    "        self.processor_base = Processor(configs, model_data['info'],\n",
    "                         model_data['model_state_dict'])\n",
    "        \n",
    "        # Select data input indices of each DNPU\n",
    "        data_input_indices_list = [[2, 3, 4]] * 3\n",
    "        ###############################################################\n",
    "        \n",
    "        # First convolution layer\n",
    "        #            nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.conv1 = DNPUConv2d(self.processor_base, data_input_indices_list,\n",
    "                               1          ,6            , 3)\n",
    "        self.bn1 = nn.BatchNorm2d(6) # Same as out channels\n",
    "        \n",
    "        # Second convolution layer\n",
    "        #            nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.conv2 = DNPUConv2d(self.processor_base, data_input_indices_list,\n",
    "                               6          ,16            , 3)\n",
    "        self.bn2 = nn.BatchNorm2d(16) # Same as out channels\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Sequential(FlattenView(), \n",
    "                                nn.Linear(400, 120),\n",
    "                                nn.ReLU(), \n",
    "                                nn.Linear(120, 84), \n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(84, 10))\n",
    "        # Activation function\n",
    "        self.act = nn.Sigmoid() # Changed activation function\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        ###############################################################\n",
    "        self.conv1.add_input_transform([0, 1])\n",
    "        self.conv2.add_input_transform([0, 1])\n",
    "        ###############################################################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x) # New line\n",
    "        x = self.act(x) \n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x) # New line\n",
    "        x = self.act(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x # Logits\n",
    "    \n",
    "    ###############################################################\n",
    "    def swap(self, configs, info):\n",
    "        self.processor_base.swap(configs, info)\n",
    "    ###############################################################\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ba88e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "model_modified = LeNetDNPU().to('cpu')\n",
    "model_modified_wrapper = TrainerMNIST(model_modified, batch_size=batch_size, learning_rate=lr).to('cpu')\n",
    "\n",
    "trainer = pl.Trainer(gpus=0, max_epochs=5,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    deterministic=True)\n",
    "\n",
    "trainer.fit(model_modified_wrapper, datamodule=mnist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
